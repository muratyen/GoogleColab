{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EuroCC-SparkML",
      "provenance": [],
      "authorship_tag": "ABX9TyMtUg4GNpU+Asj8cMvQk10h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muratyen/GoogleColab/blob/main/EuroCC_SparkML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCw7K9H6gn-K"
      },
      "source": [
        "#Spark'ın sitesinden son versiyonunu indiriyoruz\n",
        "!wget https://ftp.itu.edu.tr/Mirror/Apache/spark/spark-3.1.1/spark-3.1.1-bin-hadoop2.7.tgz\n",
        "\n",
        "#İndirdiğimiz dosyayı extract ediyoruz\n",
        "!tar -xvzf spark-3.1.1-bin-hadoop2.7.tgz\n",
        "\n",
        "#Findspark paketini kuruyoruz\n",
        "!pip install findspark\n",
        "\n",
        "#SPARK_HOME parametresine spark folder path'i veriyoruz\n",
        "import os\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop2.7/\"\n",
        "\n",
        "#Findspark'ı initialize ediyoruz\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "#Yeni bir spark session oluşturuyoruz\n",
        "from pyspark.sql import SparkSession\n",
        "sparkSession = SparkSession.builder.appName(\"PsSpark 3.0 on Google Colab\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3NLNVTfmUwV"
      },
      "source": [
        "#Uplad ettiğimiz Iris.csv dosyasını okuyup bir dataframe objesi oluşturuyoruz\n",
        "#Dosyamızın ilk satırı başlık olduğu için header parametresini true veriyoruz\n",
        "#Veri tiplerini algılaması için inferSchema parametresini true veriyoruz\n",
        "irisDF = sparkSession.read.csv(header = True, inferSchema = True, path = \"/content/data/Iris.csv\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uH6UZS3qvQj",
        "outputId": "f879e7f1-9067-4e5a-f9c6-4e3ffa5329ab"
      },
      "source": [
        "#Okuduğumuz dosyayı kontrol etmek için aşağıdaki metodu kullanıyoruz\n",
        "#5 parametresi ilk 5 kaydın listesini ekrana yazdırmak için kullanılır. Bu parametre verilmezse default 20 kayıt gelir\n",
        "irisDF.show(5)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|\n",
            "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|\n",
            "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|\n",
            "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|\n",
            "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|\n",
            "+---+-------------+------------+-------------+------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5tcA-58uZte"
      },
      "source": [
        "import pyspark.ml"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e5km470tHWK"
      },
      "source": [
        "#Veri setimizdeki kolonları tek bir vektör kolonu haline getirmek için VectorAssembler kütüphanesini kullanacağız\n",
        "#VectorAssembler kütüphanesini import ediyoruz\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#Vektör kolonunda toplanacak tüm kolonların başlıklarını inputCols parametresi ile belirtiyoruz\n",
        "#Yeni oluşacak vektör kolonunun başlık adını ise  outputCol parametresi ile belirtiyoruz\n",
        "vectorAssembler = VectorAssembler(inputCols = [\"Id\", \"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"], outputCol = \"Features\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR0JVuLmyEjj"
      },
      "source": [
        "#Vektör kolonunu oluşturmak için irisDF i transform ediyoruz\n",
        "irisDF = vectorAssembler.transform(irisDF)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAq8e1hUzDMM",
        "outputId": "a0888ca2-a512-45fb-dd99-e64a72836547"
      },
      "source": [
        "irisDF.show(5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
            "| Id|SepalLengthCm|SepalWidthCm|PetalLengthCm|PetalWidthCm|    Species|            Features|\n",
            "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
            "|  1|          5.1|         3.5|          1.4|         0.2|Iris-setosa|[1.0,5.1,3.5,1.4,...|\n",
            "|  2|          4.9|         3.0|          1.4|         0.2|Iris-setosa|[2.0,4.9,3.0,1.4,...|\n",
            "|  3|          4.7|         3.2|          1.3|         0.2|Iris-setosa|[3.0,4.7,3.2,1.3,...|\n",
            "|  4|          4.6|         3.1|          1.5|         0.2|Iris-setosa|[4.0,4.6,3.1,1.5,...|\n",
            "|  5|          5.0|         3.6|          1.4|         0.2|Iris-setosa|[5.0,5.0,3.6,1.4,...|\n",
            "+---+-------------+------------+-------------+------------+-----------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuB3rd8V0Guv"
      },
      "source": [
        "#Veri setimizdeki string kolonunu sayısal bir label kolonu haline getirmek için StringIndexer kütüphanesini kullanacağız\n",
        "#StringIndexer kütüphanesini import ediyoruz\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "#String tipinde olan kolonunun başlığını inputCol parametresi ile belirtiyoruz\n",
        "#Yeni oluşacak label kolonunun başlık adını ise outputCol parametresi ile belirtiyoruz\n",
        "stringIndexer = StringIndexer(inputCol = \"Species\", outputCol = \"Label\")"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrGpxrhj1Pa5"
      },
      "source": [
        "#Label kolonunu oluşturmak için irisDF i transform ediyoruz\n",
        "irisDF = stringIndexer.fit(irisDF).transform(irisDF)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iCPLgTl2Gx5"
      },
      "source": [
        "#Diğer kolonlara ihtiyacımız olmadığı için sadece Features ve Label kolonlarını irisDF de bırakıyoruz \n",
        "irisDF = irisDF.select(\"Features\", \"Label\")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7GIBBg33NJB"
      },
      "source": [
        "#Decision tree algoritması çalıştırmak için DecisionTreeClassifier kütüphanesini kullanacağız\n",
        "#DecisionTreeClassifier kütüphanesini import ediyoruz \n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "\n",
        "#Feature kolonunun başlığını setFeaturesCol metodu ile belirtiyoruz\n",
        "#Label kolonunun başlığını setLabelCol metodu ile belirtiyoruz\n",
        "decisionTreeClassifier = DecisionTreeClassifier().setFeaturesCol(\"Features\").setLabelCol(\"Label\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43k8p9Jy6sSC"
      },
      "source": [
        "#Decision tree modelimizi oluşturuyoruz\n",
        "model = decisionTreeClassifier.fit(irisDF)"
      ],
      "execution_count": 38,
      "outputs": []
    }
  ]
}